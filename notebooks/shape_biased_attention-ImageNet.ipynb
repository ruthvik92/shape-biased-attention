{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00588e5d-fded-47de-8285-fb30c1fe2fcb",
   "metadata": {},
   "source": [
    "  * See conversation with ChatGPT (Shape-biased Vision Transformer Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f0ff1fd-091c-481a-8589-22a32d99ab34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'/home/ruthvik/ECE-697-Fall-2022')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4363be3-cc69-4667-9346-b05219a3ae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformer.randomaug import RandAugment\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from einops import rearrange, reduce, repeat\n",
    "from transformer.Optim import ScheduledOptim\n",
    "from transformer.randomaug import RandAugment\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torchvision.datasets import ImageNet\n",
    "from cosine_annealing_warmup import CosineAnnealingWarmupRestarts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bd32d23-c4cd-4d6d-9d1d-a31d3f75ef01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:1.12.1+cu113\n",
      "Is CUDA available:True\n",
      "Device iscuda:0\n"
     ]
    }
   ],
   "source": [
    "print('PyTorch version:{}'.format(torch.__version__))\n",
    "print('Is CUDA available:{}'.format(torch.cuda.is_available()))\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device is{}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3361ada-e497-49cb-9f07-bd9c8774ae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "size = 224  # Standard ImageNet input size\n",
    "\n",
    "# Define transformations for training and test sets\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((size, size)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "N = 2\n",
    "M = 14\n",
    "transform_train.transforms.insert(0, RandAugment(N, M))\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((size, size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "# Specify the path where ImageNet is stored\n",
    "imagenet_root = \"/home/ruthvik/data/imagenet\"  # Change this to your ImageNet directory\n",
    "\n",
    "# Load datasets\n",
    "trainset = ImageNet(root=imagenet_root, split='train', transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "testset = ImageNet(root=imagenet_root, split='val', transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# ImageNet contains 1000 classes\n",
    "classes = [str(i) for i in range(1000)]  # Placeholder for class names\n",
    "\n",
    "\n",
    "#classes = ('plane', 'car', 'bird', 'cat',\n",
    "#           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8be724ac-6375-4779-a8de-cbdf005da739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples in the training set:1281167\n"
     ]
    }
   ],
   "source": [
    "print('Number of examples in the training set:{}'.format(len(trainset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0dfab98-7082-41ea-9e9e-cc4af587cb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples in the training set:50000\n"
     ]
    }
   ],
   "source": [
    "print('Number of examples in the training set:{}'.format(len(testset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42bafd39-f6ab-4d3b-a0fa-10ca44a4c17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in the training data loader:10010\n"
     ]
    }
   ],
   "source": [
    "n_batches = len(trainloader)\n",
    "print('Number of batches in the training data loader:{}'.format(n_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af61f12f-a97f-4cdb-9d7e-5fbbe852abbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels: int=3, patch_size: int=16, d_model: int=512, img_size: int=size,\n",
    "                n_conv_layers: int=1):\n",
    "        self.patch_size = patch_size\n",
    "        super().__init__()\n",
    "        # using a conv layer instead of a linear one -> performance gains\n",
    "        # same_conv_layer means the shapes of input and output images is same as opposed to valid mode conv.\n",
    "        self.same_conv_layer_stack = nn.ModuleList([nn.Conv2d(in_channels, in_channels, kernel_size=5, stride=1, padding=2) \\\n",
    "                                                    for i in range(n_conv_layers)])\n",
    "        self.conv_proj_layer = nn.Conv2d(in_channels, d_model, kernel_size=patch_size, stride=patch_size)\n",
    "        self.re_arrange_layer = Rearrange('b e (h) (w) -> b (h w) e')\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        self.position_token = nn.Parameter(torch.randn((img_size // patch_size)**2 + 1, d_model))\n",
    "        \n",
    "                \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        b, *_ = x.shape\n",
    "        for same_conv_layer in self.same_conv_layer_stack:\n",
    "            x = same_conv_layer(x)\n",
    "        convd_img = self.conv_proj_layer(x)\n",
    "        #print('Output of convolution:{}'.format(convd_img.shape))\n",
    "        re_arranged_ip = self.re_arrange_layer(convd_img)\n",
    "        #print('Rearranged ip:{}'.format(re_arranged_ip.shape))\n",
    "        cls_token = repeat(self.cls_token, '() n e -> b n e', b=b)\n",
    "        #print('CLS token:{}'.format(cls_token.shape))\n",
    "        concated_ip = torch.cat([cls_token, re_arranged_ip], axis=1)\n",
    "        concated_ip += self.position_token\n",
    "        #print('Concated ip:{}'.format(concated_ip.shape))\n",
    "        \n",
    "        return concated_ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600d82cf-5450-4491-ae45-c7e090bb7b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_img = torch.Tensor(np.random.randint(0, 255, size=(2,3, size, size)).astype(np.float32))\n",
    "print('Original image:{}'.format(src_img.shape))\n",
    "concated_ip = PatchEmbedding(n_conv_layers=2)(src_img).to(device=device)\n",
    "print('projected and cls_toen concated image:{}:{}'.format(concated_ip.shape, concated_ip.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a11bf0c-a2c8-4ced-a731-2c68a17fa688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patch_positions(img_size, patch_size, device='cuda'):\n",
    "    \"\"\"\n",
    "    Return a tensor of shape (num_patches, 2) with (row, col) indices\n",
    "    for each patch in the patch grid.\n",
    "    \"\"\"\n",
    "    H = img_size // patch_size\n",
    "    W = img_size // patch_size\n",
    "    positions = []\n",
    "    for r in range(H):\n",
    "        for c in range(W):\n",
    "            positions.append([r, c])\n",
    "    # positions is a list of length H*W, each [r, c]\n",
    "    patch_positions = torch.tensor(positions).to(device=device)  # shape (H*W, 2)\n",
    "    return patch_positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fefe208-ad58-48b4-9174-e80b681354d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 32x32images, patch_size=4 -> 8x8=64 patches\n",
    "base_positions = get_patch_positions(32, 4)  # shape (64,2) or get_patch_positions_in_original_space\n",
    "B = concated_ip.shape[0]\n",
    "positions_2d_batched = base_positions.unsqueeze(0).repeat(B, 1, 1).to(device=device)  # (B, 64, 2)\n",
    "print(positions_2d_batched.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880a5d8c-a170-4b17-8b52-bc625aa0ced9",
   "metadata": {},
   "source": [
    "# Closely examining the neighbourhood correlated patch attention penalty method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4891fa3e-9a11-4cbf-96cf-f8f15a42ff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Image size:{}, Patch_size:{}, Patch_grid_size:{}\".format((32,32), 8, (4,4)))\n",
    "print(\"\\n\")\n",
    "base_positions = get_patch_positions(32, 8)\n",
    "print(\"patch_positions_grid:\\n{}, \\n and its shape:{}\".format(base_positions, base_positions.shape))\n",
    "rows = base_positions[:,0]\n",
    "cols = base_positions[:,1]\n",
    "print(\"\\n\")\n",
    "print(\"rows:{}\".format(rows))\n",
    "print(\"cols:{}\".format(cols))\n",
    "print(\"\\n\")\n",
    "# (2) Distances\n",
    "row_diff = rows.unsqueeze(1) - rows.unsqueeze(0)  # still on GPU\n",
    "print(\"rows_diff:\\n{}\".format(row_diff))\n",
    "col_diff = cols.unsqueeze(1) - cols.unsqueeze(0)  # still on GPU\n",
    "print(\"cols_diff:\\n{}\".format(col_diff))\n",
    "dist_matrix = torch.sqrt(row_diff**2 + col_diff**2)\n",
    "print(\"dist_matrix:\\n{}\".format(dist_matrix))\n",
    "dist_weight = 1.0 / (1.0 + dist_matrix * 1.0)  # GPU\n",
    "print(\"dist_weight:\\n{}\".format(torch.round(dist_weight, decimals=2).cpu().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b11606-8c39-40dc-b1b6-d0aa63fb490b",
   "metadata": {},
   "source": [
    "* Image size:(32, 32), Patch_size:8, Patch_grid_size:(4, 4)\n",
    "\n",
    "* patch_positions_grid:\n",
    "tensor([[0, 0],\n",
    "        [0, 1],\n",
    "        [0, 2],\n",
    "        [0, 3],\n",
    "        [1, 0],\n",
    "        [1, 1],\n",
    "        [1, 2],\n",
    "        [1, 3],\n",
    "        [2, 0],\n",
    "        [2, 1],\n",
    "        [2, 2],\n",
    "        [2, 3],\n",
    "        [3, 0],\n",
    "        [3, 1],\n",
    "        [3, 2],\n",
    "        [3, 3]], device='cuda:0'), \n",
    "* and its shape:torch.Size([16, 2])\n",
    "\n",
    "* rows:tensor([0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3], device='cuda:0')\n",
    "* cols:tensor([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3], device='cuda:0')\n",
    " \n",
    "* rows_diff:\n",
    "\n",
    "        [[ 0,  0,  0,  0, -1, -1, -1, -1, -2, -2, -2, -2, -3, -3, -3, -3],\n",
    "        [ 0,  0,  0,  0, -1, -1, -1, -1, -2, -2, -2, -2, -3, -3, -3, -3],\n",
    "        [ 0,  0,  0,  0, -1, -1, -1, -1, -2, -2, -2, -2, -3, -3, -3, -3],\n",
    "        [ 0,  0,  0,  0, -1, -1, -1, -1, -2, -2, -2, -2, -3, -3, -3, -3],\n",
    "        [ 1,  1,  1,  1,  0,  0,  0,  0, -1, -1, -1, -1, -2, -2, -2, -2],\n",
    "        [ 1,  1,  1,  1,  0,  0,  0,  0, -1, -1, -1, -1, -2, -2, -2, -2],\n",
    "        [ 1,  1,  1,  1,  0,  0,  0,  0, -1, -1, -1, -1, -2, -2, -2, -2],\n",
    "        [ 1,  1,  1,  1,  0,  0,  0,  0, -1, -1, -1, -1, -2, -2, -2, -2],\n",
    "        [ 2,  2,  2,  2,  1,  1,  1,  1,  0,  0,  0,  0, -1, -1, -1, -1],\n",
    "        [ 2,  2,  2,  2,  1,  1,  1,  1,  0,  0,  0,  0, -1, -1, -1, -1],\n",
    "        [ 2,  2,  2,  2,  1,  1,  1,  1,  0,  0,  0,  0, -1, -1, -1, -1],\n",
    "        [ 2,  2,  2,  2,  1,  1,  1,  1,  0,  0,  0,  0, -1, -1, -1, -1],\n",
    "        [ 3,  3,  3,  3,  2,  2,  2,  2,  1,  1,  1,  1,  0,  0,  0,  0],\n",
    "        [ 3,  3,  3,  3,  2,  2,  2,  2,  1,  1,  1,  1,  0,  0,  0,  0],\n",
    "        [ 3,  3,  3,  3,  2,  2,  2,  2,  1,  1,  1,  1,  0,  0,  0,  0],\n",
    "        [ 3,  3,  3,  3,  2,  2,  2,  2,  1,  1,  1,  1,  0,  0,  0,  0]],\n",
    "       \n",
    "* cols_diff:\n",
    "\n",
    "        [[ 0, -1, -2, -3,  0, -1, -2, -3,  0, -1, -2, -3,  0, -1, -2, -3],\n",
    "        [ 1,  0, -1, -2,  1,  0, -1, -2,  1,  0, -1, -2,  1,  0, -1, -2],\n",
    "        [ 2,  1,  0, -1,  2,  1,  0, -1,  2,  1,  0, -1,  2,  1,  0, -1],\n",
    "        [ 3,  2,  1,  0,  3,  2,  1,  0,  3,  2,  1,  0,  3,  2,  1,  0],\n",
    "        [ 0, -1, -2, -3,  0, -1, -2, -3,  0, -1, -2, -3,  0, -1, -2, -3],\n",
    "        [ 1,  0, -1, -2,  1,  0, -1, -2,  1,  0, -1, -2,  1,  0, -1, -2],\n",
    "        [ 2,  1,  0, -1,  2,  1,  0, -1,  2,  1,  0, -1,  2,  1,  0, -1],\n",
    "        [ 3,  2,  1,  0,  3,  2,  1,  0,  3,  2,  1,  0,  3,  2,  1,  0],\n",
    "        [ 0, -1, -2, -3,  0, -1, -2, -3,  0, -1, -2, -3,  0, -1, -2, -3],\n",
    "        [ 1,  0, -1, -2,  1,  0, -1, -2,  1,  0, -1, -2,  1,  0, -1, -2],\n",
    "        [ 2,  1,  0, -1,  2,  1,  0, -1,  2,  1,  0, -1,  2,  1,  0, -1],\n",
    "        [ 3,  2,  1,  0,  3,  2,  1,  0,  3,  2,  1,  0,  3,  2,  1,  0],\n",
    "        [ 0, -1, -2, -3,  0, -1, -2, -3,  0, -1, -2, -3,  0, -1, -2, -3],\n",
    "        [ 1,  0, -1, -2,  1,  0, -1, -2,  1,  0, -1, -2,  1,  0, -1, -2],\n",
    "        [ 2,  1,  0, -1,  2,  1,  0, -1,  2,  1,  0, -1,  2,  1,  0, -1],\n",
    "        [ 3,  2,  1,  0,  3,  2,  1,  0,  3,  2,  1,  0,  3,  2,  1,  0]]\n",
    "  \n",
    "* dist_matrix:\n",
    "*     [[0.0000, 1.0000, 2.0000, 3.0000, 1.0000, 1.4142, 2.2361, 3.1623, 2.0000, 2.2361, 2.8284, 3.6056, 3.00, 3.16, 3.60, 4.2426],\n",
    "      [1.0000, 0.0000, 1.0000, 2.0000, 1.4142, 1.0000, 1.4142, 2.2361, 2.2361, 2.0000, 2.2361, 2.8284, 3.16, 3.00, 3.16, 3.6056],\n",
    "      [2.0000, 1.0000, 0.0000, 1.0000, 2.2361, 1.4142, 1.0000, 1.4142, 2.8284, 2.2361, 2.0000, 2.2361, 3.60, 3.16, 3.00, 3.1623],\n",
    "      [3.0000, 2.0000, 1.0000, 0.0000, 3.1623, 2.2361, 1.4142, 1.0000, 3.6056, 2.8284, 2.2361, 2.0000, 4.24, 3.60, 3.16, 3.0000],\n",
    "      [1.0000, 1.4142, 2.2361, 3.1623, 0.0000, 1.0000, 2.0000, 3.0000, 1.0000, 1.4142, 2.2361, 3.1623, 2.00, 2.23, 2.82, 3.6056],\n",
    "      [1.4142, 1.0000, 1.4142, 2.2361, 1.0000, 0.0000, 1.0000, 2.0000, 1.4142, 1.0000, 1.4142, 2.2361, 2.23, 2.00, 2.23, 2.8284],\n",
    "      [2.2361, 1.4142, 1.0000, 1.4142, 2.0000, 1.0000, 0.0000, 1.0000, 2.2361, 1.4142, 1.0000, 1.4142, 2.82, 2.23, 2.00, 2.2361],\n",
    "      [3.1623, 2.2361, 1.4142, 1.0000, 3.0000, 2.0000, 1.0000, 0.0000, 3.1623, 2.2361, 1.4142, 1.0000, 3.60, 2.82, 2.23, 2.0000],\n",
    "      [2.0000, 2.2361, 2.8284, 3.6056, 1.0000, 1.4142, 2.2361, 3.1623, 0.0000, 1.0000, 2.0000, 3.0000, 1.00, 1.41, 2.23, 3.1623],\n",
    "      [2.2361, 2.0000, 2.2361, 2.8284, 1.4142, 1.0000, 1.4142, 2.2361, 1.0000, 0.0000, 1.0000, 2.0000, 1.41, 1.00, 1.41, 2.2361],\n",
    "      [2.8284, 2.2361, 2.0000, 2.2361, 2.2361, 1.4142, 1.0000, 1.4142, 2.0000, 1.0000, 0.0000, 1.0000, 2.23, 1.41, 1.00, 1.4142],\n",
    "      [3.6056, 2.8284, 2.2361, 2.0000, 3.1623, 2.2361, 1.4142, 1.0000, 3.0000, 2.0000, 1.0000, 0.0000, 3.16, 2.23, 1.41, 1.0000],\n",
    "      [3.0000, 3.1623, 3.6056, 4.2426, 2.0000, 2.2361, 2.8284, 3.6056, 1.0000, 1.4142, 2.2361, 3.1623, 0.00, 1.00, 2.00, 3.0000],\n",
    "      [3.1623, 3.0000, 3.1623, 3.6056, 2.2361, 2.0000, 2.2361, 2.8284, 1.4142, 1.0000, 1.4142, 2.2361, 1.00, 0.00, 1.00, 2.0000],\n",
    "      [3.6056, 3.1623, 3.0000, 3.1623, 2.8284, 2.2361, 2.0000, 2.2361, 2.2361, 1.4142, 1.0000, 1.4142, 2.00, 1.00, 0.00, 1.0000],\n",
    "      [4.2426, 3.6056, 3.1623, 3.0000, 3.6056, 2.8284, 2.2361, 2.0000, 3.1623, 2.2361, 1.4142, 1.0000, 3.00, 2.00, 1.00, 0.0000]],\n",
    "* Each entry, `(i,j)` in `dist_matrix` yields how far patch `i` is from the patch `j` in `patch_positions_grid` $\\in R ^{4 \\times 4}$.  \n",
    "* dist_weight: $\\frac{1.0}{1.0 + dist\\_matrix}$\n",
    "*     [[1.   0.5  0.33 0.25 0.5  0.41 0.31 0.24 0.33 0.31 0.26 0.22 0.25 0.24 0.22 0.19]\n",
    "      [0.5  1.   0.5  0.33 0.41 0.5  0.41 0.31 0.31 0.33 0.31 0.26 0.24 0.25 0.24 0.22]\n",
    "      [0.33 0.5  1.   0.5  0.31 0.41 0.5  0.41 0.26 0.31 0.33 0.31 0.22 0.24 0.25 0.24]\n",
    "      [0.25 0.33 0.5  1.   0.24 0.31 0.41 0.5  0.22 0.26 0.31 0.33 0.19 0.22 0.24 0.25]\n",
    "      [0.5  0.41 0.31 0.24 1.   0.5  0.33 0.25 0.5  0.41 0.31 0.24 0.33 0.31 0.26 0.22]\n",
    "      [0.41 0.5  0.41 0.31 0.5  1.   0.5  0.33 0.41 0.5  0.41 0.31 0.31 0.33 0.31 0.26]\n",
    "      [0.31 0.41 0.5  0.41 0.33 0.5  1.   0.5  0.31 0.41 0.5  0.41 0.26 0.31 0.33 0.31]\n",
    "      [0.24 0.31 0.41 0.5  0.25 0.33 0.5  1.   0.24 0.31 0.41 0.5  0.22 0.26 0.31 0.33]\n",
    "      [0.33 0.31 0.26 0.22 0.5  0.41 0.31 0.24 1.   0.5  0.33 0.25 0.5  0.41 0.31 0.24]\n",
    "      [0.31 0.33 0.31 0.26 0.41 0.5  0.41 0.31 0.5  1.   0.5  0.33 0.41 0.5 0.41 0.31]\n",
    "      [0.26 0.31 0.33 0.31 0.31 0.41 0.5  0.41 0.33 0.5  1.   0.5  0.31 0.41 0.5  0.41]\n",
    "      [0.22 0.26 0.31 0.33 0.24 0.31 0.41 0.5  0.25 0.33 0.5  1.   0.24 0.31 0.41 0.5 ]\n",
    "      [0.25 0.24 0.22 0.19 0.33 0.31 0.26 0.22 0.5  0.41 0.31 0.24 1.   0.5 0.33 0.25]\n",
    "      [0.24 0.25 0.24 0.22 0.31 0.33 0.31 0.26 0.41 0.5  0.41 0.31 0.5  1. 0.5  0.33]\n",
    "      [0.22 0.24 0.25 0.24 0.26 0.31 0.33 0.31 0.31 0.41 0.5  0.41 0.33 0.5 1.   0.5 ]\n",
    "      [0.19 0.22 0.24 0.25 0.22 0.26 0.31 0.33 0.24 0.31 0.41 0.5  0.25 0.33 0.5  1.  ]]\n",
    "\n",
    "\n",
    "* `penalty = alpha_tensor * corr_matrix * dist_weight`\n",
    "*  $corr\\_matrix \\in R^{16\\times16}$ why ?\n",
    "*  We have 16 patches and assuming $d\\_model \\in R^{512}$, we have features in $R^{16 \\times 512}$. Correlation is calculated as $R^{16 \\times 512} \\times R^{512 \\times 16} \\in R^{16x16} $. Let this matrix be `corr_matrix`.\n",
    "* First row in `corr_matrix` tells you how correlated each a patch is w.r.t the first patch.\n",
    "*  Operation `corr_matrix * dist_weight` will scale the correlation values inversely proportional to the distance from the patch.\n",
    "*  Note that we typically zero out the diagonal entries of `penalty` to prevent unnecessarily penalizing the self attention for each patch. \n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f15ac7d-f86b-45e8-b0a8-de5801fb2466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_shape_bias_penalty_batched(\n",
    "    patch_positions: torch.Tensor,   # (B, N, 2)\n",
    "    patch_embeddings: torch.Tensor,  # (B, N, d)\n",
    "    alpha: float = 0.5, \n",
    "    dist_scale: float = 1.0\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes a shape-bias penalty in batch. Returns shape (B, N, N).\n",
    "    \n",
    "    patch_positions[b, i] = (row_i, col_i) for the i-th patch of the b-th image.\n",
    "    patch_embeddings[b, i] = d-dim embedding for the i-th patch of the b-th image.\n",
    "    \"\"\"\n",
    "\n",
    "    device = patch_positions.device\n",
    "    dtype = patch_embeddings.dtype\n",
    "\n",
    "    # Convert scalars alpha, dist_scale into Tensors on the same device/dtype\n",
    "    alpha_tensor = torch.tensor(alpha, device=device, dtype=dtype)\n",
    "    dist_scale_tensor = torch.tensor(dist_scale, device=device, dtype=dtype)\n",
    "\n",
    "    # ---------------\n",
    "    # (1) Correlation\n",
    "    # ---------------\n",
    "    # patch_embeddings: (B, N, d)\n",
    "    # Normalize each patch embedding in dimension d to get cos similarity\n",
    "    norms = patch_embeddings.norm(dim=2, keepdim=True) + 1e-6  # (B, N, 1)\n",
    "    normalized = patch_embeddings / norms                      # (B, N, d)\n",
    "\n",
    "    # We want pairwise dot products => shape (B, N, N)\n",
    "    # Using einsum: (B,N,d) x (B,N,d) -> (B,N,N)\n",
    "    corr_matrix = torch.einsum('b i d, b j d -> b i j', normalized, normalized)\n",
    "\n",
    "    # -------------------\n",
    "    # (2) Pairwise Distances\n",
    "    # -------------------\n",
    "    # patch_positions: (B, N, 2)\n",
    "    # For row coords: shape (B, N), do unsqueeze to get (B, N, 1) - (B, 1, N) => (B, N, N)\n",
    "    row_coords = patch_positions[..., 0]  # (B, N)\n",
    "    col_coords = patch_positions[..., 1]  # (B, N)\n",
    "\n",
    "    row_diff = row_coords.unsqueeze(2) - row_coords.unsqueeze(1)  # (B, N, N)\n",
    "    col_diff = col_coords.unsqueeze(2) - col_coords.unsqueeze(1)  # (B, N, N)\n",
    "    dist_matrix = torch.sqrt(row_diff**2 + col_diff**2)           # (B, N, N)\n",
    "\n",
    "    # distance-based weight\n",
    "    dist_weight = 1.0 / (1.0 + dist_scale_tensor * dist_matrix)   # (B, N, N)\n",
    "\n",
    "    # ---------------\n",
    "    # (3) Combine\n",
    "    # ---------------\n",
    "    penalty = alpha_tensor * corr_matrix * dist_weight  # (B, N, N)\n",
    "\n",
    "    # ---------------\n",
    "    # (4) Zero out diagonal\n",
    "    # ---------------\n",
    "    # We need to zero each batch's (N, N) diagonal\n",
    "    B, N, _ = penalty.shape\n",
    "    diag_idx = torch.arange(N, device=device)\n",
    "    # We'll do this in a loop or fancy indexing. \n",
    "    # \"penalty[b, diag, diag] = 0\" for each b in [0..B-1]\n",
    "    penalty[:, diag_idx, diag_idx] = 0.0\n",
    "    #print(penalty_batched.median(), penalty_batched.max(), penalty_batched.min())\n",
    "    return penalty  # (B, N, N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4247076-28a4-4c5c-bae8-c503d5e1243b",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_embeddings_wo_cls = concated_ip[:, 1:, :] # (B, N, d)\n",
    "\n",
    "# Then call the new vectorized function\n",
    "penalty_batched = compute_shape_bias_penalty_batched(\n",
    "    positions_2d_batched,\n",
    "    patch_embeddings_wo_cls,\n",
    "    alpha=0.1,\n",
    "    dist_scale=1.0\n",
    ")  # (B, N, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32aca83-e625-4a38-9adb-c9454b7e891c",
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty_batched.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8735e64-6031-447f-8251-e9fe3d2a00a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    ''' Scaled Dot-Product Attention with optional shape-bias penalty. '''\n",
    "\n",
    "    def __init__(self, temperature, attn_dropout=0.1, alpha=1.0, dist_scale=1.0):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "        self.alpha = alpha\n",
    "        self.dist_scale = dist_scale\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        q, k, v,                 # q, k, v: (B, heads, N, d)\n",
    "        mask=None,               # optional attention mask\n",
    "        patch_positions=None,    # (B, N, 2) or (N, 2) if single-image\n",
    "        patch_embeddings=None    # (B, N, d) or (N, d)\n",
    "    ):\n",
    "        \"\"\"\n",
    "        q, k, v : (B, H, N, D)\n",
    "        patch_positions: for shape bias (B, N, 2) if you have a batch\n",
    "        patch_embeddings: for shape bias (B, N, E) or (B, N, D)\n",
    "        \"\"\"\n",
    "\n",
    "        # (B, H, N, d) x (B, H, d, N) -> (B, H, N, N)\n",
    "        attn_logits = torch.matmul(q / self.temperature, k.transpose(2, 3))\n",
    "\n",
    "        B, H, N, _ = q.shape\n",
    "\n",
    "        # ---------------------------------------------------\n",
    "        # (1) Optionally compute shape-bias penalty per batch\n",
    "        # ---------------------------------------------------\n",
    "        if patch_positions is not None and patch_embeddings is not None:\n",
    "            # We'll assume patch_positions and patch_embeddings are batched\n",
    "            # shapes: (B, N, 2), (B, N, D)\n",
    "            # We'll produce a penalty matrix for each item in batch\n",
    "            # penalty_matrices = []\n",
    "            # for b_idx in range(B):\n",
    "            #     # shape: (N,2) and (N,d)\n",
    "            #     pos_b = patch_positions[b_idx]\n",
    "            #     emb_b = patch_embeddings[b_idx]\n",
    "            #     # compute penalty for this sample\n",
    "            #     M = compute_shape_bias_penalty(\n",
    "            #         pos_b, emb_b[1:,:], alpha=self.alpha, dist_scale=self.dist_scale\n",
    "            #     )\n",
    "            #     # M is (N, N)\n",
    "            #     penalty_matrices.append(M.unsqueeze(0))  # -> (1, N, N)\n",
    "\n",
    "            # # Stack into (B, N, N) and expand heads: (B, 1, N, N) -> (B, H, N, N)\n",
    "            # penalty_full = torch.stack(penalty_matrices, dim=0)\n",
    "            # #print(penalty_full.shape)  ##<---- 2x1x64x64\n",
    "            # #penalty_full = penalty_full.unsqueeze(1).expand(-1, H, -1, -1)\n",
    "            # penalty_full = penalty_full.expand(-1, H, -1, -1)\n",
    "            \n",
    "            # # Subtract from attn_logits\n",
    "            # # attn_logits shape = (B, H, N+1, N+1)\n",
    "            # # penalty_full shape = (B, H, N, N)\n",
    "            # attn_logits[..., 1:, 1:] = attn_logits[..., 1:, 1:] - penalty_full\n",
    "            # #print(attn_logits.shape, penalty_full.shape)\n",
    "            # #attn_logits = attn_logits - penalty_full\n",
    "            patch_embeddings_wo_cls = patch_embeddings[:, 1:, :] # (B, N, d)\n",
    "\n",
    "            # Then call the new vectorized function\n",
    "            penalty_batched = compute_shape_bias_penalty_batched(\n",
    "                patch_positions,\n",
    "                patch_embeddings_wo_cls,\n",
    "                alpha=self.alpha,\n",
    "                dist_scale=self.dist_scale\n",
    "            )  # (B, N, N)\n",
    "            # Suppose penalty.shape = (B, N, N)\n",
    "            # You can expand to (B, H, N, N)\n",
    "            penalty_expanded = penalty_batched.unsqueeze(1).expand(-1, H, -1, -1)  # (B, H, N, N)\n",
    "            #print(\"penalty\")\n",
    "            #print(penalty_expanded.cpu().max(), penalty_expanded.cpu().min(), penalty_expanded.cpu().median())\n",
    "            #print(attn_logits.cpu().max(), attn_logits.cpu().min(), attn_logits.cpu().median())\n",
    "            attn_logits[..., 1:, 1:] -= penalty_expanded  # if you're skipping CLS in the attention\n",
    "            #print(attn_logits.max(), attn_logits.median(), attn_logits.min())\n",
    "\n",
    "        # -----------------------------------------------\n",
    "        # (2) If you have a mask, apply it here\n",
    "        # -----------------------------------------------\n",
    "        if mask is not None:\n",
    "            # mask shape is typically (B, 1, N, N) or (B, H, N, N), \n",
    "            # with 0/1 entries\n",
    "            attn_logits = attn_logits.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # -----------------------------------------------\n",
    "        # (3) Softmax over last dim\n",
    "        # -----------------------------------------------\n",
    "        attn = F.softmax(attn_logits, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        # -----------------------------------------------\n",
    "        # (4) Multiply by v to get final output\n",
    "        # -----------------------------------------------\n",
    "        output = torch.matmul(attn, v)  # (B, H, N, D)\n",
    "\n",
    "        return output  # or (output, attn) if you want the attention map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab8a2ff0-47f5-40f2-b525-41f6c9e20c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    ''' Multi-Head Attention module '''\n",
    "\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1, alpha=1.0, dist_scale=1.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "        self.w_qs = nn.Linear(d_model, n_head * d_k, bias=False)\n",
    "        self.w_ks = nn.Linear(d_model, n_head * d_k, bias=False)\n",
    "        self.w_vs = nn.Linear(d_model, n_head * d_v, bias=False)\n",
    "        self.fc = nn.Linear(n_head * d_v, d_model, bias=False)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(temperature=d_k ** 0.5, alpha=alpha, dist_scale=dist_scale)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "\n",
    "    def forward(self, q, k, v, mask=None, patch_positions=None, patch_embeddings=None):    \n",
    "                                          # (B, N, 2) or (N, 2) if single-image\n",
    "\n",
    "        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n",
    "        sz_b, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)\n",
    "\n",
    "        residual = q\n",
    "\n",
    "        # Pass through the pre-attention projection: b x lq x (n*dv) = (1 x 65 x 512)\n",
    "        # Separate different heads: b x lq x n x dv\n",
    "        #print(q.shape)\n",
    "        #print(self.w_qs.weight.shape)\n",
    "        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k) #(1 x 65 x 512) --> (1 x 65 x 8 x 64)\n",
    "        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k) #(1 x 65 x 512) --> (1 x 65 x 8 x 64)\n",
    "        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v) #(1 x 65 x 512) --> (1 x 65 x 8 x 64)\n",
    "\n",
    "        # Transpose for attention dot product: b x n x lq x dv = (1 x 8 x 65 x 64)\n",
    "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)   # For head axis broadcasting.\n",
    "\n",
    "        #q, attn = self.attention(q, k, v, mask=mask) # (1 x 8 x 65 x 64) and (1 x 8 x 65 x 65)\n",
    "        q  = self.attention(q, k, v, mask=mask, patch_positions=patch_positions, patch_embeddings=patch_embeddings) # (10 x 8 x 12 x 64) and (10 x 8 x 12 x 12)\n",
    "        # Transpose to move the head dimension back: b x lq x n x dv\n",
    "        # Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)\n",
    "        # q.transpose(1, 2) --> (1 x 65 x 8 x 64)\n",
    "        q = q.transpose(1, 2).contiguous().view(sz_b, len_q, -1) #(1 x 65 x 512)\n",
    "        q = self.dropout(self.fc(q)) ## (1 x 65 x 512) x (512 x 512) --> (1 x 65 x 512)\n",
    "        q += residual\n",
    "\n",
    "        q = self.layer_norm(q)\n",
    "\n",
    "        #return q, attn\n",
    "        return q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833f3c84-4c24-4cfb-a538-4f1fd47da700",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_head = 8\n",
    "n_layers = 6\n",
    "d_k = 64\n",
    "d_v = 64\n",
    "batch = 1\n",
    "d_inner = 512\n",
    "d_model = 512\n",
    "q = concated_ip\n",
    "k = q\n",
    "v = q\n",
    "print('Input to the multi head attention:{}'.format(q.shape))\n",
    "multi_head_block = MultiHeadAttention(n_head=n_head, d_model=d_model, d_k=d_k, d_v=d_v,alpha=0.1,).to(device=device)\n",
    "#q, self_attn = multi_head_block(q, k ,v)  \n",
    "#patch_embeddings = concated_ip[:, 1:, :]\n",
    "q = multi_head_block(q, k ,v, patch_positions=positions_2d_batched, patch_embeddings=concated_ip, )\n",
    "#print('Shape of q:{}, Shape of self attention:{}'.format(q.shape, self_attn.shape))\n",
    "print('Shape of q:{}'.format(q.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac1c84c4-e212-43a2-bf48-266960d82d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    ''' A two-feed-forward-layer module '''\n",
    "\n",
    "    def __init__(self, d_in, d_hid, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.w_1 = nn.Linear(d_in, d_hid) # position-wise\n",
    "        self.w_2 = nn.Linear(d_hid, d_in) # position-wise\n",
    "        self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        residual = x\n",
    "        x = self.w_2(F.relu(self.w_1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x += residual\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "513246e0-271e-432b-a95d-3e134a44f9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    ''' Compose with two layers '''\n",
    "\n",
    "    def __init__(self, d_model, d_inner, n_head, d_k, d_v, dropout=0.1,alpha=1.0,):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout,alpha=alpha)\n",
    "        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)\n",
    "\n",
    "    def forward(self, enc_input, slf_attn_mask=None, patch_positions=None, patch_embeddings=None):\n",
    "        #enc_output, enc_slf_attn = self.slf_attn(\n",
    "        #    enc_input, enc_input, enc_input, mask=slf_attn_mask)\n",
    "        enc_output = self.slf_attn(\n",
    "            enc_input, enc_input, enc_input, mask=slf_attn_mask, patch_positions=patch_positions, patch_embeddings=patch_embeddings)\n",
    "        if(type(enc_output) == tuple):\n",
    "            enc_output, enc_slf_attn = enc_output\n",
    "        enc_output = self.pos_ffn(enc_output)\n",
    "        #return enc_output, enc_slf_attn\n",
    "        return enc_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7aea1bc4-d9f9-4789-9c80-502007330132",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    ''' A encoder model with self attention mechanism. '''\n",
    "\n",
    "    def __init__(\n",
    "            self, n_layers, n_head, d_k, d_v,\n",
    "            d_model, d_inner, dropout=0.1, n_conv_layers=1,alpha=1.0,):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.position_enc = PatchEmbedding(d_model=d_model, n_conv_layers=n_conv_layers,patch_size=16, img_size=224)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.layer_stack = nn.ModuleList([EncoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout,alpha=alpha) \\\n",
    "                                          for _ in range(n_layers)])\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.d_model = d_model\n",
    "        self.positions_2d = get_patch_positions(img_size=224, patch_size=16, device=device)\n",
    "\n",
    "    def forward(self, input_image, return_attns=False):\n",
    "        B = input_image.shape[0]\n",
    "\n",
    "        enc_slf_attn_list = []\n",
    "\n",
    "        # -- Forward\n",
    "        patch_embedding = self.position_enc(input_image)\n",
    "        #print(patch_embedding.shape)\n",
    "        enc_output = self.dropout(patch_embedding)\n",
    "        enc_output = self.layer_norm(enc_output)\n",
    "        positions_2d_batched = self.positions_2d.unsqueeze(0).repeat(B, 1, 1)  # (B, 64, 2)\n",
    "\n",
    "        for enc_layer in self.layer_stack:\n",
    "            #enc_output, enc_slf_attn = enc_layer(enc_output, slf_attn_mask=None)\n",
    "            #enc_slf_attn_list += [enc_slf_attn] if return_attns else []\n",
    "            enc_output = enc_layer(enc_output, slf_attn_mask=None, patch_positions=positions_2d_batched, patch_embeddings=patch_embedding)\n",
    "\n",
    "        #if return_attns:\n",
    "        #    return enc_output, enc_slf_attn_list\n",
    "        return enc_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3978fcc-3977-4279-bfc3-e876d8435115",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_head = 8\n",
    "n_layers = 6\n",
    "d_k = 64\n",
    "d_v = 64\n",
    "d_inner = 512\n",
    "d_model = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cabb98-ce3d-4397-be4c-7c976f33be26",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_img = torch.Tensor(np.random.randint(0, 255, size=(2,3, 32, 32))).to(device=device)\n",
    "print('Original image:{}'.format(src_img.shape))\n",
    "encoder = Encoder(n_layers=6, n_head=6, d_k=d_k, d_v=d_v, d_model=d_model, d_inner=d_inner, n_conv_layers=2).to(device=device)\n",
    "enc_output = encoder(input_image=src_img)\n",
    "print('Encoder output:{}'.format(enc_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1841a645-a61d-4cea-b74a-d3a13ade68cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHeadClsTokenPooling(nn.Module):\n",
    "    def __init__(self, d_model: int = 512, n_classes: int = 1000, cls_token_pos: int = 0):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(d_model) \n",
    "        self.linear_layer = nn.Linear(d_model, n_classes)\n",
    "        self.cls_token_pos = cls_token_pos\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        reduced_encoder_op = x[:,self.cls_token_pos,:]\n",
    "        #print('Reduced encoder shape:{}'.format(reduced_encoder_op.shape))\n",
    "        layer_normed_reduced = self.layer_norm(reduced_encoder_op)\n",
    "        output = self.linear_layer(layer_normed_reduced)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5269c85-6b40-491a-ba9d-cad477584148",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHeadWithAvgPooling(nn.Module):\n",
    "    def __init__(self, d_model: int = 512, n_classes: int = 1000):\n",
    "        super().__init__()\n",
    "        self.reduction_layer = Reduce('b n e -> b e', reduction='mean')\n",
    "        self.layer_norm = nn.LayerNorm(d_model) \n",
    "        self.linear_layer = nn.Linear(d_model, n_classes)\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        reduced_encoder_op = self.reduction_layer(x)\n",
    "        #print('Reduced encoder shape:{}'.format(reduced_encoder_op.shape))\n",
    "        layer_normed_reduced = self.layer_norm(reduced_encoder_op)\n",
    "        output = self.linear_layer(layer_normed_reduced)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48860d7a-d171-4d85-b7c8-63a42b2fe077",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_head = ClassificationHeadWithAvgPooling().to(device=device)\n",
    "print('Input to the classifier:{}'.format(enc_output.shape))\n",
    "classification_op = classification_head(enc_output)\n",
    "print('Classification output:{}'.format(classification_op.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab4de232-52ba-441e-bd0d-23ff46d12ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, n_layers, n_head, d_k, d_v, d_model, d_inner, n_classes, \n",
    "                dropout=0.1, n_conv_layers=1,alpha=1.0,):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(n_layers=n_layers, n_head=n_head, d_k=d_k, d_v=d_v, d_model=d_model, \n",
    "                               d_inner=d_inner, n_conv_layers=n_conv_layers,alpha=alpha)\n",
    "        #self.classifier_head = ClassificationHeadWithAvgPooling(d_model=d_model, n_classes=n_classes)\n",
    "        self.classifier_head = ClassificationHeadClsTokenPooling(d_model=d_model, n_classes=n_classes)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        #print(x.shape)\n",
    "        #encoder_op = self.encoder(input_image=x)\n",
    "        encoder_op = self.encoder(x)\n",
    "        classifier_op = self.classifier_head(encoder_op)\n",
    "        return classifier_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c4499e8-25d7-4779-9f92-16f7d46098ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_head = 8\n",
    "n_layers = 6\n",
    "d_k = 64\n",
    "d_v = 64\n",
    "batch = 1\n",
    "d_inner = 512\n",
    "d_model = 512\n",
    "n_classes= 1000\n",
    "alpha=1.0\n",
    "n_conv_layers = 4\n",
    "vit = ViT(n_layers=n_layers, n_head=n_head, d_k=d_k, d_v=d_v, d_model=d_model, d_inner=d_inner,\n",
    "         n_classes=n_classes, n_conv_layers=n_conv_layers,alpha=alpha,).to(device)\n",
    "#vit = torch.nn.DataParallel(vit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2495f66-bb10-432e-8b09-8cfcfba9396f",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_img = torch.Tensor(np.random.randint(0, 255, size=(2,3, size, size))).to(device)\n",
    "print('Original image:{}'.format(src_img.shape))\n",
    "vit_op = vit(src_img)\n",
    "print('vit output:{}'.format(vit_op.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf481a0c-4aae-4018-aeba-9b5b020e7034",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary(vit, (3, 224, 224), device='cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9e5692a-0859-4126-8d66-7a070dfe3ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of batches(steps):100100\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "gamma = 0.75\n",
    "n_steps = epochs*n_batches\n",
    "print('Total number of batches(steps):{}'.format(n_steps))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#lr = 0.0005"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2381d1c5-fc20-403b-8046-298dcd480c8e",
   "metadata": {},
   "source": [
    "optimizer = optim.SGD(vit.parameters(), lr=lr, momentum=0.9)\n",
    "first_cycle_steps=int(n_batches*12) ## after 50 epochs, a cycle is completed.\n",
    "print('Steps (batches) for the first cycle to complete:{}'.format(first_cycle_steps))\n",
    "min_lr = lr / 6\n",
    "print('Minimum learning rate:{} and maximum learning rate:{} for this scheduler'.format(min_lr, lr))\n",
    "warmup_steps = int(first_cycle_steps / 2.0)\n",
    "print('Warmup steps:{}'.format(warmup_steps))\n",
    "scheduler = CosineAnnealingWarmupRestarts(optimizer, first_cycle_steps=first_cycle_steps, cycle_mult=1.0, \n",
    "                                          max_lr=lr, min_lr=min_lr, warmup_steps=warmup_steps, gamma=gamma) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a313630-535f-4145-a072-47fd5759bbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use AdamW instead of SGD for better performance with ViTs\n",
    "lr = 3e-4  # Standard learning rate for AdamW\n",
    "#optimizer = optim.AdamW(vit.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=0.05)\n",
    "\n",
    "# No scheduler (constant learning rate)\n",
    "#scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: 1.0)\n",
    "\n",
    "\n",
    "def warmup_constant_lr(step):\n",
    "    warmup_steps = 5 * len(trainloader)  # Warmup for 5 epochs\n",
    "    if step < warmup_steps:\n",
    "        return step / warmup_steps  # Gradual increase\n",
    "    return 1.0  # Constant after warmup\n",
    "\n",
    "optimizer = optim.AdamW(vit.parameters(), lr=3e-4)\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=warmup_constant_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "884c20da-894c-4e75-bd9b-e3fc3025593b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruthvik/pytorch_1121_cu113/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Learning rate')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrs = []\n",
    "for i in range(n_steps):\n",
    "    lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "    scheduler.step()\n",
    "plt.plot(list(range(len(lrs))), lrs)\n",
    "plt.grid()\n",
    "plt.xlabel('Steps (batches)')\n",
    "plt.ylabel('Learning rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748a68b5-8e17-4de9-9d2b-f198748d5791",
   "metadata": {},
   "source": [
    "scheduler = CosineAnnealingWarmupRestarts(optimizer, first_cycle_steps=first_cycle_steps, cycle_mult=1.0, \n",
    "                                          max_lr=lr, min_lr=min_lr, warmup_steps=warmup_steps, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "491f4dd9-4ec3-43c3-9db3-774183a76924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No scheduler (constant learning rate)\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=warmup_constant_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db914320-12d2-441b-90d1-5ea11ddcb638",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './imagent_shape_biased_net_{}extra_conv_sgd_lr_{}_gamma_{}_alpha_{}_{}_epochs_randaugs_{}_batch.pth'.format(n_conv_layers,\n",
    "                                                                                      lr, gamma, alpha, epochs, batch_size)\n",
    "print('Checkpoints will be saved to:{}'.format(PATH))\n",
    "def save_checkpoint(model, path):\n",
    "    print('Saved checkpoint to:{}'.format(path))\n",
    "    torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f3189ab-28e5-4aac-b9c0-1dd1f4cce4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoints will be saved to:./imagent_shape_biased_net_4extra_conv_adam_lr_0.0003_alpha_1.0_10_epochs_randaugs_128_batch.pth\n"
     ]
    }
   ],
   "source": [
    "PATH = './imagent_shape_biased_net_{}extra_conv_adam_lr_{}_alpha_{}_{}_epochs_randaugs_{}_batch.pth'.format(n_conv_layers,\n",
    "                                                                                      lr, alpha, epochs, batch_size)\n",
    "print('Checkpoints will be saved to:{}'.format(PATH))\n",
    "def save_checkpoint(model, path):\n",
    "    print('Saved checkpoint to:{}'.format(path))\n",
    "    torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c5931f6-b5f4-4a74-87f2-1c87db606110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = vit(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print('Accuracy of the network on the {} test images: {}'.format(total, accuracy))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c5fe50-1f15-491c-bee9-cb7b5467e3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 10010] loss: 6.781163\n",
      "Learning rate:5.999400599400599e-05\n",
      "Accuracy of the network on the 50000 test images: 2.306\n",
      "Saved checkpoint to:./imagent_shape_biased_net_4extra_conv_adam_lr_0.0003_alpha_1.0_10_epochs_randaugs_128_batch.pth\n",
      "Time elapsed for epoch:1 is:6286.145447254181\n",
      "[2, 10010] loss: 5.889519\n",
      "Learning rate:0.00011999400599400598\n",
      "Accuracy of the network on the 50000 test images: 8.242\n",
      "Saved checkpoint to:./imagent_shape_biased_net_4extra_conv_adam_lr_0.0003_alpha_1.0_10_epochs_randaugs_128_batch.pth\n",
      "Time elapsed for epoch:2 is:6288.678830623627\n",
      "[3, 10010] loss: 4.951574\n",
      "Learning rate:0.00017999400599400596\n",
      "Accuracy of the network on the 50000 test images: 17.272\n",
      "Saved checkpoint to:./imagent_shape_biased_net_4extra_conv_adam_lr_0.0003_alpha_1.0_10_epochs_randaugs_128_batch.pth\n",
      "Time elapsed for epoch:3 is:6263.684277534485\n"
     ]
    }
   ],
   "source": [
    "best_test_accuracy = 0\n",
    "test_accuracy_history = []\n",
    "lrs_history = []\n",
    "train_loss_history = []\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    t1 = time.time()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = vit(inputs)\n",
    "        #log_prb = F.log_softmax(outputs, dim=1)\n",
    "        #loss = F.cross_entropy(log_prb, labels, reduction='sum')\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #optimizer.step_and_update_lr()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % n_batches == (n_batches-1):    # print every 2000 mini-batches\n",
    "            curr_train_loss = running_loss / (n_batches)\n",
    "            print('[%d, %5d] loss: %.6f' %\n",
    "                  (epoch + 1, i + 1, curr_train_loss))\n",
    "            train_loss_history.append(curr_train_loss)\n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            #lr = optimizer._optimizer.param_groups[0][\"lr\"]\n",
    "            lrs_history.append(lr)\n",
    "            print('Learning rate:{}'.format(lr))\n",
    "            running_loss = 0.0\n",
    "        \n",
    "        if i % (n_batches) == (n_batches-1):\n",
    "            curr_test_accuracy = test_accuracy()\n",
    "            test_accuracy_history.append(curr_test_accuracy)\n",
    "            if(curr_test_accuracy > best_test_accuracy):\n",
    "                best_test_accuracy = curr_test_accuracy\n",
    "                save_checkpoint(vit, PATH)\n",
    "        scheduler.step()\n",
    "        #warmup_scheduler.dampen()\n",
    "    t2 = time.time()\n",
    "    print('Time elapsed for epoch:{} is:{}'.format(epoch+1, t2-t1))\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a261e29-843d-4f57-b914-e75349bd8245",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = ViT(n_layers=n_layers, n_head=n_head, d_k=d_k, d_v=d_v, d_model=d_model, d_inner=d_inner,\n",
    "         n_classes=n_classes, n_conv_layers=n_conv_layers,alpha=alpha,).to(device)\n",
    "#vit = torch.nn.DataParallel(vit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1b7e9d-04e9-4394-bbf3-2377d802e99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6134dd-6f95-43a7-a455-e99d6308e3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PATH =  './cifar_net_2extra_conv_sgd_lr_0.05_500_epochs_randaugs_100_batch.pth'\n",
    "vit.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f286df7a-6c66-43f2-8e6c-b125da81d521",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracies = []\n",
    "n_trials = 50\n",
    "t1 = time.time()\n",
    "for i in range(n_trials):\n",
    "    test_acc = test_accuracy()\n",
    "    test_accuracies.append(test_acc)\n",
    "print('Time taken:{}'.format(time.time() - t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fb3059-7442-4db0-8c1a-b592b7d92bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Max accuracy is:{}, min is:{}, and median:{}'.format(np.max(test_accuracies),\\\n",
    "                                                            np.min(test_accuracies),\\\n",
    "                                                            np.median(test_accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a390ba12-5b03-4af1-975c-cd00ef2af589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = vit(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname,\n",
    "                                                   accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1c1784-a439-43f2-9245-a8d011b85a66",
   "metadata": {},
   "source": [
    "# Best accuracy setting so far"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd04c9b-29ad-4eb3-8f26-432bfd30eddf",
   "metadata": {},
   "source": [
    "**Trial 1**\n",
    "* './cifar_shape_biased_net_4extra_conv_sgd_lr_0.03_gamma_0.88_alpha_1.0_500_epochs_randaugs_100_batch.pth'\n",
    "\n",
    "* Max accuracy is:91.16, min is:90.51, and median:90.72\n",
    "```\n",
    "epochs = 500\n",
    "gamma = 0.88\n",
    "n_steps = epochs*n_batches\n",
    "print('Total number of batches(steps):{}'.format(n_steps))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 0.03\n",
    "optimizer = optim.SGD(vit.parameters(), lr=lr, momentum=0.9)\n",
    "first_cycle_steps=int(n_batches*50) ## after 50 epochs, a cycle is completed.\n",
    "print('Steps (batches) for the first cycle to complete:{}'.format(first_cycle_steps))\n",
    "min_lr = lr / 6\n",
    "print('Minimum learning rate:{} and maximum learning rate:{} for this scheduler'.format(min_lr, lr))\n",
    "warmup_steps = int(first_cycle_steps / 2.0)\n",
    "print('Warmup steps:{}'.format(warmup_steps))\n",
    "scheduler = CosineAnnealingWarmupRestarts(optimizer, first_cycle_steps=first_cycle_steps, cycle_mult=1.0, \n",
    "                                          max_lr=lr, min_lr=min_lr, warmup_steps=warmup_steps, gamma=gamma) \n",
    "\n",
    "n_head = 8\n",
    "n_layers = 6\n",
    "d_k = 64\n",
    "d_v = 64\n",
    "batch = 1\n",
    "d_inner = 512\n",
    "d_model = 512\n",
    "n_classes= 10\n",
    "alpha=1.0\n",
    "n_conv_layers = 4\n",
    "vit = ViT(n_layers=n_layers, n_head=n_head, d_k=d_k, d_v=d_v, d_model=d_model, d_inner=d_inner,\n",
    "         n_classes=n_classes, n_conv_layers=n_conv_layers,alpha=alpha,).to(device)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19daa8ea-4939-4b12-8e80-82aecb87ad3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_1121_cu_113",
   "language": "python",
   "name": "pytorch_1121_cu_113"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
